<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="The Role of Consequential and Functional Sound in Human–Robot Interaction: Toward Audio-Augmented Reality Interfaces.">
  <meta name="keywords" content="Audio augmented reality, Functional Sounds, Consequential Sounds, Human-Robot Interaction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The Role of Consequential and Functional Sound in Human–Robot Interaction: Toward Audio-Augmented Reality Interfaces</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://aliyah-smith.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://aliyah-smith.github.io/audioar.github.io">
            AudioAR
          <!-- </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape -->
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio-Augmented Reality Interfaces</h1>
          <div class="is-size-4 publication-authors">
            <span class="author-block">
              <a href="https://aliyah-smith.github.io">Aliyah Smith</a><sup>1</sup> and </span>
            <span class="author-block">
              <a href="https://monroekennedy3.com/">Monroe Kennedy III</a><sup>1</sup></span>
          </div>

          <div class="is-size-4 publication-authors">
            <span class="author-block"><sup>1</sup>Stanford University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2511.15956"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2511.15956"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/cS0-iCbRCsg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/preview.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        This website gives an overview of our paper, which explores the role of nonverbal sound in human-robot interaction.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            As robots become increasingly integrated into everyday environments, 
            understanding how they communicate with humans is critical. 
            Sound offers a powerful channel for interaction, 
            encompassing both operational noises and intentionally designed auditory cues. 
          </p>
          <p>
            
            In this three-part study, we examined the effects of consequential and functional sounds on human perception and behavior, 
            including a novel exploration of spatial sound through localization and handover tasks. The first part utilized a 
            between-subjects design (N=48) to assess how the presence of consequential sounds from a Kinova Gen3 robotic manipulator 
            influences human perceptions of the robot (in person and through video) during a simple pick and place task. The second part
            (N=51) investigated spatial sound localization accuracy using a augmented reality (AR) environment, 
            where participants identified the source of 3D sounds. The third part (N=41) evaluated the impact of 
            functional and spatial auditory cues on user experience and perception of the robot through a within-subjects design.
            
          </p>
          <p>
            Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, 
            spatial localization is highly accurate for lateral cues but declines for frontal cues, 
            and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. 
            These findings highlight the potential of functional and transformative auditory design to enhance human-robot 
            collaboration and inform future sound-based interaction strategies.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/cS0-iCbRCsg?si=X9GojxuGBylNqX7D"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Recruitment -->
      <div class="column">
        <h2 class="title is-3">Participant Recruitment</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Fifty-one participants were recruited under IRB protocol 65022. 
              Informed consent was obtained prior to participation, 
              and each session lasted approximately 45 minutes. 
              Participant demographics are summarized below. 
              Likert-scale questions (1 = Strongly Disagree, 7 = Strongly Agree) 
              assessed participants' experience with and attitudes toward robots.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Demographics-->
<section class="hero is-light is-small">
  <div class="hero-body">
    <!-- <div class="container"> -->
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-gender">
        <img src="./static/images/gender_distribution.png"
            class="demographics-image"
            alt="Gender distribution."/>
      </div>
      <div class="item item-experience">
        <img src="./static/images/experience_likert.png"
            class="demographics-image"
            alt="Experience with robots."/>
      </div>
      <div class="item item-comfort">
        <img src="./static/images/comfort_likert.png"
            class="demographics-image"
            alt="Comfort with robots."/>
      </div>
      <div class="item item-trust">
        <img src="./static/images/trust_likert.png"
            class="demographics-image"
            alt="Trust in robots."/>
      </div>
      <div class="item item-music">
        <img src="./static/images/musical_experience.png"
            class="demographics-image"
            alt="Music or sound experience."/>
      </div>
      <div class="item item-age">
        <img src="./static/images/age_distribution.png"
            class="demographics-image"
            alt="Age distribution."/>
      </div>
      <!-- </div> -->
    </div>
  </div>
</section>
<!--/ Demographics-->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
    <!-- Experiment A. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiment A: An In-person Replication of the Study on the Effects of Consequential Sounds on Human Perception of Robots </h2>

        <!-- Hypotheses. -->
        <h3 class="title is-4">Hypotheses</h3>
        <div class="content has-text-justified">
          <p>
             <strong>H1:</strong> When observing the robot with sound (through recording or collocation), 
            participants will exhibit more negative associated effects,
            report lower levels of liking, and express a reduced desire for physical co-location.
          </p>
          <p>
            <strong>H2:</strong> Human perceptions of the robot when exposed to consequential sounds 
            through video recordings will be more positive overall compared to those of 
            participants directly colocated with the robot.
          </p>
        </div>
        <!-- <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Hypotheses. -->

        <!-- Experimental Design -->
        <h3 class="title is-4">Experimental Design</h3>
        <div class="content has-text-justified">
          <p>
            To establish a baseline understanding of how sound influences human perceptions of
            the Kinova Gen3 manipulator, we replicated a previously con-
            ducted between-subjects study that examined similar effects
            using online videos and surveys. The primary objective
            was to determine whether consequential sounds elicit negative
            perceptions toward this specific robot and to assess potential
            differences between participants who were co-located with the
            robot and those who observed it through video recordings.
          </p>
          <p>
            Accordingly, we designed four experimental conditions (shown below).
            Across all four conditions, the Kinova manipulator executed
            a standardized pick-and-place task (video below). Participants were not provided
            any prior information about the task or its purpose before
            observing the robot. Furthermore, the true objective of the
            study was withheld to minimize potential bias related to sound
            perception.
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/part_1_conditions.png"
                class="part1-methods-image"
                alt="Part 1 methods image."/>
          <p class="is-bold">The four experimental conditions for Experiment A are shown. The
            Kinova Gen3 manipulator was selected for its suitability in collaborative and
            household tasks. Participants were assigned to conditions
            using a quasi-random procedure to ensure equal group sizes.</p>
        </div>
        <div class="content has-text-centered">
          <video id="part1-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/kinova.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Experimental Design. -->

        <!-- Results -->
        <h3 class="title is-4">Results</h3>
        <div class="content has-text-justified">
          <p>
            After observing the robot, participants completed a 11-item Likert-scale questionnaire assessing their perceptions of the robot. 
            The questions measured four perceptual scales: Associated Affect, Distraction, Liking, and Physical Co-location Desire.
            Those assigned to the sound conditions were additionally asked questions specific
            to the robot's auditory characteristics. 
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/part_1_likert_box_whisker_means.png"
                class="part1-results-image"
                alt="Part 1 results image."/>
          <p class="is-bold">Box-and-whisker plots illustrating participant responses across the four
            experimental conditions and four perceptual scales in Experiment A. 
            Higher scores indicate more positive perceptions. Black diamonds represent mean values,
            black horizontal lines indicate medians, plus signs denote outliers, 
            boxes correspond to the interquartile range (25th-75th percentiles), 
            and whiskers extend to 1.5 times the interquartile range. (N = 48)</p>
        </div>
        <div class="content has-text-justified">
          <p>
            One-way ANOVAs revealed no significant differences between the four conditions across all perceptual scales:
            Associated Effect (p = 0.1256), Distracted (p = 1.0000), Colocate (p = 0.7323), and Like (p = 2.662). 
            Trends in the data, from less positive to more positive perception, were as follows:
          </p>

          <ul>
            <li><strong>Associated Effect:</strong> In-person, Sound &lt; Recorded, Muted &lt; Recorded, Sound &lt; In-person, Muted (means: 5.58, 5.60, 6.12, 6.48)</li>
            <li><strong>Distracted:</strong> In-person, Muted &lt; In-person, Sound &lt; Recorded, Muted &lt; Recorded, Sound (means: 4.75, 4.92, 5.00, 5.08)</li>
            <li><strong>Colocate:</strong> In-person, Sound &lt; Recorded, Muted &lt; In-person, Muted &lt; Recorded, Sound (means: 3.92, 4.03, 4.33, 4.61)</li>
            <li><strong>Like:</strong> Recorded, Muted &lt; In-person, Sound &lt; In-person, Muted &lt; Recorded, Sound (means: 4.36, 5.03, 5.06, 5.19)</li>
          </ul>

        </div>
        <!--/ Results. -->
      </div>
    </div>
    <!--/ Experiment A. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiment B. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiment B: A Study on Spatial Sound Localization </h2>
        <!-- Hypotheses. -->
        <h3 class="title is-4">Hypotheses</h3>
        <div class="content has-text-justified">
          <p>
            <strong>H3:</strong> Participants will more accurately distinguish static
            sounds originating from their left or right sides (i.e., at larger
            azimuth angles) than those coming from directly in front of
            them.
          </p>
        </div>
        <!-- <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Hypotheses. -->

        <!-- Experimental Design -->
        <h3 class="title is-4">Experimental Design</h3>
        <div class="content has-text-justified">
          <p>
            To investigate participants' ability to discriminate spatial sounds and guide our spatial sound design,
             we conducted an AR experiment. Participants sat across from the robot while 
             wearing a Microsoft HoloLens 2, which rendered spatialized 360° audio. 
             A custom mixed-reality application presented three red spheres in the participants'
             field of view, approximately spanning the width of the robot's workspace.
          </p>
          <p>
            Participants then experienced two additional scenes: 
            one with three green spheres and another with five blue spheres. 
            In each scene, they identified the locations of 3D audio sources, 
            visually represented by the corresponding spheres. For Scenes 1 and 2, 
            audio sources were initially presented sequentially with concurrent visual feedback 
            (spheres oscillating with the sounds) and repeated twice. Visual cues were then removed, 
            and sounds were presented in random order. After each sound, 
            participants verbally indicated the corresponding sphere. 
            This procedure was repeated over two trials, with each sound played once per trial. 
            In Scene 3, participants identified audio sources without prior visual feedback.
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/part2_scenes.png"
                class="part2-methods-image"
                alt="Part 2 methods image."/>
          <p class="is-bold">The three virtual scenes, in which spheres represent the positions of the static spatial sounds.</p>
        </div>
        <div class="content has-text-centered">
          <video id="part1-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/scene1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Experimental Design. -->

        <!-- Results -->
        <h3 class="title is-4">Results</h3>
        <div class="content has-text-justified">
          <p>
            For each scene, the true sequence of audio sources and the corresponding participant-identified 
            (predicted) sequence were recorded across both trials.
            
            Quantitative results were analyzed using confusion matrices 
            that compared the predicted and true labels for
            each scene across all participants. These matrices were used
            to compute classification accuracy and examine error patterns,
            providing insight into overall spatial sound localization per-
            formance and potential sources of misclassification.
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/part_2_confusion_matrices_borders.png"
                class="part2-results-image"
                alt="Part 2 results image."/>
          <p class="is-bold">Normalized confusion matrices aggregated for all participants across three scenes.</p>
        </div>
        <div class="content has-text-justified">
          <p>
            Participants' accuracy generally
            declined as scene complexity increased. Accuracy decreased
            slightly from Trial 1 to Trial 2 in Scene 1, whereas it
            showed slight improvements in Scenes 2 and 3 across trials.
            Specifically, overall accuracy was 95% and 93% for Scene 1
            (Trials 1 and 2, respectively), 84% and 86% for Scene 2, and
            74% and 78% for Scene 3.
          </p>
        </div>
        <!--/ Results. -->
      </div>
    </div>
    <!--/ Experiment B. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiment C. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiment C: Exploring Functional Sounds in Human-Robot Collaboration </h2>
        <!-- Hypotheses. -->
        <h3 class="title is-4">Hypotheses</h3>
        <div class="content has-text-justified">
          <p>
            <strong>H4:</strong> Adding functional sound will increase participants'
            feelings of competence and reduce participants' feelings of
            discomfort compared to consequential sounds alone.
          </p>
          <p>
            <strong>H5:</strong> Adding spatial sound will increase participants' feelings
            of warmth and competence, and reduce participants' feelings
            of discomfort compared to consequential sounds alone.
          </p>
          <p>
            <strong>H6:</strong> Participants will accurately interpret the intended mean-
            ing of the functional sounds presented.
          </p>
        </div>
        <!-- <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Hypotheses. -->

        <!-- Experimental Design -->
        <h3 class="title is-4">Experimental Design</h3>
        <div class="content has-text-justified">
          <p>
            Participants were tasked with completing a Lego structure over the course
            of three trials, with the robot assisting by providing additional
            Lego pieces as needed. Each trial was conducted under a distinct sound condition: 
            Consequential, Functional, or Spatial.
          </p>
          <p>
            Participants were informed that they would complete a brief
            survey after each trial to provide feedback on the robot and
            the associated sounds; no additional information about the
            sound conditions was provided to prevent participants from
            focusing explicitly on the auditory stimuli. Following the three
            trials, participants completed a post-experiment questionnaire,
            reflecting on their experiences and offering opinions and
            recommendations for each of the three sound conditions from
            memory. Towards the conclusion of the survey, the two aug-
            mented sound conditions were replayed, and the experimenter
            provided an explanation of their functional design. Finally,
            participants indicated their preferred sound condition and
            offered any additional comments regarding the sound designs.
          </p>
        </div>
        <!-- <div class="content has-text-centered">
          <video id="part1-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/scene1.mp4"
                    type="video/mmp4">
          </video>
        </div> -->
        <!--/ Experimental Design. -->

        <!-- Results -->
        <h3 class="title is-4">Results</h3>
        <div class="content has-text-justified">
          <p>
            Participants completed the Robot Social Attribute Scale (RoSAS) after each trial and completed a custom
            survey at the end of the experiment. The RoSAS is a validated tool for 
            assessing social perceptions of robots across three dimensions: Warmth, Competence, and Discomfort.
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/part_3_rosas_box_whisker_excl_means.png"
                class="part3-results-image"
                alt="Part 3 results image."/>
          <p class="is-bold">Box-and-whisker plots illustrating participant responses across the three experimental 
            conditions and three attribute scales in Experiment C. Statistical significance from 
            exploratory paired comparisons is indicated by asterisks above brackets (*corresponds to p &lt; 0.05). (N = 41)</p>
        </div>
        <div class="content has-text-justified">
          <p>
           <p>
            Analyses from the post-trial surveys indicated no statistically significant differences across the three attribute scales:
            Warmth (p = 0.205), Competence (p = 0.384), and Discomfort (p = 0.081). 
            Exploratory post-hoc paired comparisons suggested trends toward differences between specific conditions.
          </p>

          <p>
            Specifically, the Consequential and Spatial sound conditions showed a trend toward higher Warmth (p = 0.090) and lower Discomfort (p = 0.042) in the Spatial condition, 
            while the Functional and Spatial sound conditions showed a trend toward lower Discomfort (p = 0.052) in the Spatial condition. 
            Although these findings should be interpreted cautiously, given the non-significant overall tests and uncorrected multiple comparisons, 
            they point to potential differences in how sound design influences participants' perceptions and may guide future research.
          </p>

          <p>Trends in the data were as follows:</p>

          <ul>
            <li><strong>Warmth:</strong> Consequential &lt; Functional &lt; Spatial (means: 1.99, 2.01, 2.49)</li>
            <li><strong>Competence:</strong> Consequential &lt; Functional &lt; Spatial (means: 3.47, 3.57, 3.91)</li>
            <li><strong>Discomfort:</strong> Spatial &lt; Consequential &lt; Functional (means: 1.41, 1.70, 1.74)</li>
          </ul>

          </p>
        </div>
        <div class="column has-text-justified">
          </p>
            The following images summarize additional results from Experiment C:
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/part_3_likert_stacked_bar.png"
                class="part3-results-image"
                alt="Part 3 results image."/>
          <p class="is-bold">Likert responses for the augmented sound conditions.</p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/part_3_table.PNG"
                class="part3-results-image"
                alt="Part 3 results image."/>
          <!-- <p class="is-bold">Thematic.</p> -->
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/part_3_sound_condition_preferences.png"
                class="part3-results-image"
                alt="Part 3 results image."/>
          <!-- <p class="is-bold">Thematic.</p> -->
        </div>
        <!-- <p>
            Additonal qualitative analyses of the post-experiment survey responses revealed that participants generally
          </p> -->
        <!--/ Results. -->
      </div>
    </div>
    <!--/ Experiment C. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Additional Data. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Additional Results </h2>
        <h3 class="title is-4">Design Implications</h3>
        <div class="content has-text-justified">
          <p>
           Additional data were collected to inform future sound design for human-robot interaction.
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/part4_importance_heatmap_combined_ordered.png"
                class="part4-methods-image"
                alt="Part 4 methods image."/>
          <p class="is-bold">Heatmaps depicting participants' ratings of the importance of communicating four 
            distinct categories of information through sound. The left panel shows overall ratings across all 
            participants, the middle panel shows ratings from participants reporting low experience with robots, 
            and the right panel shows ratings from participants reporting high experience with robots. (N = 41) </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{smith2025roleconsequentialfunctionalsound,
      title={The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces}, 
      author={Aliyah Smith and Monroe Kennedy III},
      year={2025},
      eprint={2511.15956},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2511.15956}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website uses the Nerfies <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
